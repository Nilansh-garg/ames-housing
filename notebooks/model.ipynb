{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7db5efa4-dc55-44c6-b15e-3c6f6db0f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e89f798-65d5-4ddd-aa58-98c791951a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train_input.csv', na_values = ['','NaN','NA'], keep_default_na = False)\n",
    "df_test_file = pd.read_csv('data/test_input.csv',na_values = ['','NaN','NA'], keep_default_na = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfc90316-b573-43a5-92dd-71cff2ce4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = df['Sale Price'].copy()\n",
    "target_test = df_test_file['Sale Price'].copy()\n",
    "\n",
    "train_features = df.drop(\"Sale Price\", axis = 1)\n",
    "test_features = df_test_file.drop(\"Sale Price\", axis = 1)\n",
    "\n",
    "num_features = train_features.select_dtypes(exclude = 'object').columns.tolist()\n",
    "cat_features = train_features.select_dtypes(include = 'object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e9fc025-f0fc-40ab-a782-dd43ebc25dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler = StandardScaler()\n",
    "num_pipeline = Pipeline(\n",
    "    steps = [\n",
    "    (\"imputing\",SimpleImputer(strategy = \"mean\")),\n",
    "    ]\n",
    ")\n",
    "cat_pipeline = Pipeline(\n",
    "    steps = [\n",
    "    (\"Imputing\",SimpleImputer(strategy = \"most_frequent\")),\n",
    "    (\"Encoder\",OneHotEncoder(handle_unknown = 'ignore'))\n",
    "    ]\n",
    ")\n",
    "full_pipeline = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\",num_pipeline,num_features),\n",
    "        (\"cat\",cat_pipeline,cat_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "084d72cf-ed0e-4fa1-b0a3-23e3cbf34690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = full_pipeline.fit_transform(train_features)\n",
    "df_test = full_pipeline.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23202d9b-3e73-4e41-9bd7-c29938af65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true, predicted):\n",
    "    mae = mean_absolute_error(true,predicted)\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(mean_squared_error(true, predicted))\n",
    "    r2_Score = r2_score(true, predicted)\n",
    "    return mae, rmse, r2_Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a98fba2-805c-484e-9b0b-2f637990a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"lasso\":Lasso(),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"KNeighbors Regressor\": KNeighborsRegressor(),\n",
    "    \"Ridge\":Ridge(),\n",
    "    \"Random Forest\":RandomForestRegressor(),\n",
    "    \"Decision Tree\":DecisionTreeRegressor(),\n",
    "    \"Ada Boost Regressor\":AdaBoostRegressor(),\n",
    "    \"Xgb boost\":XGBRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8b84216-de57-4ffc-9164-cb2d997fcd94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso\n",
      "- Model performance for Training set\n",
      "- Root Mean Squared Error:0.2384\n",
      "- Mean Absolute Error:0.1683\n",
      "- R2 Score: 0.6620\n",
      "----------------------------------------\n",
      "- Model performance for Test set\n",
      "- Root Mean Squared Error :0.2308\n",
      "- Mean Absolute Error:0.1676\n",
      "- R2 Score: 0.6626\n",
      "\n",
      "Linear Regression\n",
      "- Model performance for Training set\n",
      "- Root Mean Squared Error:0.2877\n",
      "- Mean Absolute Error:0.2158\n",
      "- R2 Score: 0.5078\n",
      "----------------------------------------\n",
      "- Model performance for Test set\n",
      "- Root Mean Squared Error :0.2884\n",
      "- Mean Absolute Error:0.2176\n",
      "- R2 Score: 0.4732\n",
      "\n",
      "KNeighbors Regressor\n",
      "- Model performance for Training set\n",
      "- Root Mean Squared Error:0.1914\n",
      "- Mean Absolute Error:0.1320\n",
      "- R2 Score: 0.7821\n",
      "----------------------------------------\n",
      "- Model performance for Test set\n",
      "- Root Mean Squared Error :0.2192\n",
      "- Mean Absolute Error:0.1567\n",
      "- R2 Score: 0.6957\n",
      "\n",
      "Ridge\n",
      "- Model performance for Training set\n",
      "- Root Mean Squared Error:0.3962\n",
      "- Mean Absolute Error:0.3063\n",
      "- R2 Score: 0.0661\n",
      "----------------------------------------\n",
      "- Model performance for Test set\n",
      "- Root Mean Squared Error :0.3814\n",
      "- Mean Absolute Error:0.2969\n",
      "- R2 Score: 0.0785\n",
      "\n",
      "Random Forest\n",
      "- Model performance for Training set\n",
      "- Root Mean Squared Error:0.0527\n",
      "- Mean Absolute Error:0.0340\n",
      "- R2 Score: 0.9835\n",
      "----------------------------------------\n",
      "- Model performance for Test set\n",
      "- Root Mean Squared Error :0.1309\n",
      "- Mean Absolute Error:0.0894\n",
      "- R2 Score: 0.8915\n",
      "\n",
      "Decision Tree\n",
      "- Model performance for Training set\n",
      "- Root Mean Squared Error:0.0000\n",
      "- Mean Absolute Error:0.0000\n",
      "- R2 Score: 1.0000\n",
      "----------------------------------------\n",
      "- Model performance for Test set\n",
      "- Root Mean Squared Error :0.1756\n",
      "- Mean Absolute Error:0.1286\n",
      "- R2 Score: 0.8047\n",
      "\n",
      "Ada Boost Regressor\n",
      "- Model performance for Training set\n",
      "- Root Mean Squared Error:0.1570\n",
      "- Mean Absolute Error:0.1251\n",
      "- R2 Score: 0.8533\n",
      "----------------------------------------\n",
      "- Model performance for Test set\n",
      "- Root Mean Squared Error :0.1630\n",
      "- Mean Absolute Error:0.1275\n",
      "- R2 Score: 0.8317\n",
      "\n",
      "Xgb boost\n",
      "- Model performance for Training set\n",
      "- Root Mean Squared Error:0.0158\n",
      "- Mean Absolute Error:0.0114\n",
      "- R2 Score: 0.9985\n",
      "----------------------------------------\n",
      "- Model performance for Test set\n",
      "- Root Mean Squared Error :0.1245\n",
      "- Mean Absolute Error:0.0880\n",
      "- R2 Score: 0.9018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = []\n",
    "r2_list = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(df_train, target_train)\n",
    "\n",
    "    y_train_pred = model.predict(df_train)\n",
    "    y_test_pred = model.predict(df_test)\n",
    "\n",
    "    model_train_mae, model_train_rmse, model_train_r2 = evaluate_model(target_train, y_train_pred)\n",
    "    model_test_mae, model_test_rmse, model_test_r2 = evaluate_model(target_test, y_test_pred)\n",
    "\n",
    "    print(list(models.keys())[i])\n",
    "    model_list.append(list(models.keys())[i])\n",
    "    \n",
    "    print(\"- Model performance for Training set\")\n",
    "    print(\"- Root Mean Squared Error:{:.4f}\".format(model_train_rmse))\n",
    "    print(\"- Mean Absolute Error:{:.4f}\".format(model_train_mae))\n",
    "    print(\"- R2 Score: {:.4f}\".format(model_train_r2))\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    \n",
    "    print('- Model performance for Test set')\n",
    "    print(\"- Root Mean Squared Error :{:.4f}\".format(model_test_rmse))\n",
    "    print(\"- Mean Absolute Error:{:.4f}\".format(model_test_mae))\n",
    "    print(\"- R2 Score: {:.4f}\".format(model_test_r2))\n",
    "    print()\n",
    "    r2_list.append(model_test_r2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5681cfe-0c3c-4c7f-8ca5-749d72a41982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
      "[CV 1/5] END colsample_bytree=0.749816047538945, gamma=0.4753571532049581, learning_rate=0.1, max_depth=7, min_child_weight=5, reg_alpha=0.15601864044243652, reg_lambda=0.15599452033620265, subsample=0.6232334448672797;, score=(train=0.925, test=0.903) total time=   1.2s\n",
      "[CV 2/5] END colsample_bytree=0.749816047538945, gamma=0.4753571532049581, learning_rate=0.1, max_depth=7, min_child_weight=5, reg_alpha=0.15601864044243652, reg_lambda=0.15599452033620265, subsample=0.6232334448672797;, score=(train=0.928, test=0.875) total time=   1.4s\n",
      "[CV 3/5] END colsample_bytree=0.749816047538945, gamma=0.4753571532049581, learning_rate=0.1, max_depth=7, min_child_weight=5, reg_alpha=0.15601864044243652, reg_lambda=0.15599452033620265, subsample=0.6232334448672797;, score=(train=0.923, test=0.907) total time=   1.3s\n",
      "[CV 4/5] END colsample_bytree=0.749816047538945, gamma=0.4753571532049581, learning_rate=0.1, max_depth=7, min_child_weight=5, reg_alpha=0.15601864044243652, reg_lambda=0.15599452033620265, subsample=0.6232334448672797;, score=(train=0.930, test=0.847) total time=   1.5s\n",
      "[CV 5/5] END colsample_bytree=0.749816047538945, gamma=0.4753571532049581, learning_rate=0.1, max_depth=7, min_child_weight=5, reg_alpha=0.15601864044243652, reg_lambda=0.15599452033620265, subsample=0.6232334448672797;, score=(train=0.925, test=0.905) total time=   1.1s\n",
      "[CV 1/5] END colsample_bytree=0.9464704583099741, gamma=0.3005575058716044, learning_rate=0.2, max_depth=5, min_child_weight=6, reg_alpha=0.056411579027100256, reg_lambda=0.7219987722668247, subsample=0.9754210836063001;, score=(train=0.938, test=0.905) total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.9464704583099741, gamma=0.3005575058716044, learning_rate=0.2, max_depth=5, min_child_weight=6, reg_alpha=0.056411579027100256, reg_lambda=0.7219987722668247, subsample=0.9754210836063001;, score=(train=0.940, test=0.880) total time=   1.1s\n",
      "[CV 3/5] END colsample_bytree=0.9464704583099741, gamma=0.3005575058716044, learning_rate=0.2, max_depth=5, min_child_weight=6, reg_alpha=0.056411579027100256, reg_lambda=0.7219987722668247, subsample=0.9754210836063001;, score=(train=0.937, test=0.903) total time=   1.1s\n",
      "[CV 4/5] END colsample_bytree=0.9464704583099741, gamma=0.3005575058716044, learning_rate=0.2, max_depth=5, min_child_weight=6, reg_alpha=0.056411579027100256, reg_lambda=0.7219987722668247, subsample=0.9754210836063001;, score=(train=0.939, test=0.841) total time=   1.2s\n",
      "[CV 5/5] END colsample_bytree=0.9464704583099741, gamma=0.3005575058716044, learning_rate=0.2, max_depth=5, min_child_weight=6, reg_alpha=0.056411579027100256, reg_lambda=0.7219987722668247, subsample=0.9754210836063001;, score=(train=0.940, test=0.902) total time=   1.3s\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint, uniform as sp_uniform\n",
    "\n",
    "# 1. Define the XGBoost Regressor (or XGBClassifier for classification)\n",
    "# Set common parameters and choose a high n_estimators (trees) \n",
    "# as the search will find the optimal number of *effective* trees.\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',  # Loss function for regression\n",
    "    random_state=42, \n",
    "    n_estimators=1000,             # Start high, will be controlled by learning_rate\n",
    "    n_jobs=-1                      # Use all cores\n",
    ")\n",
    "\n",
    "# 2. Define the Parameter Search Space (Distributions)\n",
    "# Use distributions from scipy.stats for continuous variables in RandomizedSearchCV\n",
    "param_dist = {\n",
    "    # Tree Complexity (to combat the overfitting you saw)\n",
    "    'max_depth': sp_randint(3, 8),          # Lower the depth (e.g., 3 to 7)\n",
    "    'min_child_weight': sp_randint(1, 10),  # Increase the minimum samples required for a split\n",
    "    'gamma': sp_uniform(0.0, 0.5),          # Increase min loss reduction required for a split\n",
    "    \n",
    "    # Randomness/Subsampling (further regularization)\n",
    "    'subsample': sp_uniform(0.6, 0.4),      # Sample from 60% to 100% (0.6 + 0.4) of rows\n",
    "    'colsample_bytree': sp_uniform(0.6, 0.4), # Sample from 60% to 100% of columns\n",
    "    \n",
    "    # Learning/Boosting\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2], # Smaller rates are often better\n",
    "    'reg_alpha': sp_uniform(0.0, 1.0),      # L1 regularization\n",
    "    'reg_lambda': sp_uniform(0.0, 1.0)      # L2 regularization\n",
    "}\n",
    "\n",
    "# 3. Initialize and Run the Search\n",
    "# X_train and y_train are your training data (e.g., NumPy arrays or Pandas DataFrames)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,                  # Number of parameter settings that are sampled (tune this!)\n",
    "    scoring='r2',               # Use R-squared for evaluation\n",
    "    cv=5,                       # Use 5-fold cross-validation\n",
    "    verbose=3,\n",
    "    random_state=42,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Fit the search to your data\n",
    "random_search.fit(df_train, target_train)\n",
    "\n",
    "# 5. Get the Best Model\n",
    "best_xgb_model = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4850ccf-fae1-4bcc-b5c6-b7e9b2d6e4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=np.float64(0.7644148053272926), device=None,\n",
      "             early_stopping_rounds=None, enable_categorical=False,\n",
      "             eval_metric=None, feature_types=None, feature_weights=None,\n",
      "             gamma=np.float64(0.016525366450274193), grow_policy=None,\n",
      "             importance_type=None, interaction_constraints=None,\n",
      "             learning_rate=0.2, max_bin=None, max_cat_threshold=None,\n",
      "             max_cat_to_onehot=None, max_delta_step=None, max_depth=3,\n",
      "             max_leaves=None, min_child_weight=1, missing=nan,\n",
      "             monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n",
      "             n_jobs=-1, num_parallel_tree=None, ...)\n"
     ]
    }
   ],
   "source": [
    "print(best_xgb_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a569584f-5648-40af-9daa-c3db5e235a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Tuned Model R2 on Test Set: 0.9768077696341226\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'random_search' object from the previous fit is still in memory\n",
    "\n",
    "# 1. Retrieve the optimal model object\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "\n",
    "# 2. Evaluate the model on the unseen test set\n",
    "# This provides the final, non-cross-validated R2 score for reporting.\n",
    "test_r2_score = best_xgb_model.score(df_train, target_train)\n",
    "\n",
    "print(f\"Final Tuned Model R2 on Test Set: {test_r2_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "791313d9-7acf-45c4-a789-cf3b97712dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully saved to best_xgb_model_tuned.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming 'best_xgb_model' is the object you retrieved from random_search.best_estimator_\n",
    "\n",
    "# Define the filename\n",
    "filename = 'best_xgb_model_tuned.joblib'\n",
    "\n",
    "# Save the model to file\n",
    "joblib.dump(best_xgb_model, filename)\n",
    "\n",
    "print(f\"Model successfully saved to {filename}\")\n",
    "\n",
    "# --- To Load the model later ---\n",
    "# loaded_model = joblib.load(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
