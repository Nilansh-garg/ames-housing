import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedShuffleSplit


df = pd.read_csv("data/AmesHousing.csv",na_values =['','NA'], keep_default_na = False)


df.info()


df.nunique()


drop_columns = ['Alley','Pool QC','Pool Area','Misc Feature','Misc Val','Fence','Screen Porch','3Ssn Porch','Enclosed Porch','SalePrice']
drop_columns1 = ['Alley','Pool QC','Pool Area','Misc Feature','Misc Val','Fence','SalePrice']


df_train = df.drop(columns = drop_columns).copy()
df_train_1 = df.drop(columns = drop_columns1)
target = df['SalePrice']


df_train.nunique()
    


df_train.describe()





df.hist(bins = 50, figsize = (12,20))


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the simplified dataset


# 1. Prepare the Target Variable (Log-Transformation)
df['SalePrice'] = np.log1p(df['SalePrice'])

# 2. Re-create the key engineered feature 'Total SF'
df['Total SF'] = df['Total Bsmt SF'] + df['1st Flr SF'] + df['2nd Flr SF']

# 3. Define features for visualization (a sample of the most influential columns)
numerical_cols_to_plot = ['Gr Liv Area', 'Total SF', 'Lot Area', 'Lot Frontage']
categorical_cols_to_plot = ['Overall Qual', 'Neighborhood', 'Kitchen Qual']

# ----------------------------------------
# Figure 1: Numerical Features vs. SalePrice (Scatter Plots)
# ----------------------------------------
plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols_to_plot):
    plt.subplot(2, 2, i + 1)
    # Use Log_SalePrice for clearer linearity
    sns.scatterplot(x=df[col], y=df['SalePrice'])
    plt.title(f'{col} vs. Log(SalePrice)')
    plt.xlabel(col)
    plt.ylabel('Log(SalePrice)')
    plt.tight_layout()
plt.suptitle('Impact of Key Numerical Features on Sale Price', y=1.02, fontsize=16)
plt.savefig('numerical_feature_impact.png')
plt.close()

# ----------------------------------------
# Figure 2: Categorical Features vs. SalePrice (Box Plots)
# ----------------------------------------
plt.figure(figsize=(15, 8))
for i, col in enumerate(categorical_cols_to_plot):
    plt.subplot(1, 3, i + 1)
    # Ensure correct order for ordinal data like Overall Qual, Kitchen Qual
    if col in ['Overall Qual', 'Kitchen Qual']:
        if col == 'Overall Qual':
            order = sorted(df['Overall Qual'].unique())
        else: # Kitchen Qual has specific levels
            quality_order = ['Po', 'Fa', 'TA', 'Gd', 'Ex', 'None']
            order = [q for q in quality_order if q in df[col].unique()]
    else:
        # Sort nominal features by median SalePrice for better visualization
        order = df.groupby(col)['SalePrice'].median().sort_values(ascending=False).index

    sns.boxplot(x=df[col], y=df['SalePrice'], order=order, fliersize=1)
    plt.title(f'{col} vs. Log(SalePrice)')
    plt.xlabel(col, rotation=45, ha='right')
    plt.ylabel('Log(SalePrice)')
    plt.tight_layout()

plt.suptitle('Impact of Key Categorical Features on Sale Price', y=1.05, fontsize=16)
plt.savefig('categorical_feature_impact.png')



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the simplified dataset (AmesHousing_Simplified_Features.csv)


# 1. Prepare the necessary features
df['Log_SalePrice'] = np.log1p(df['SalePrice'])
df['Total SF'] = df['Total Bsmt SF'] + df['1st Flr SF'] + df['2nd Flr SF']

# 2. Generate the Regression Plot
plt.figure(figsize=(10, 6))
sns.regplot(
    x='Total SF',
    y='Log_SalePrice',
    data=df,
    scatter_kws={'alpha': 0.6},
    line_kws={'color': 'red'},
    ci=95  # This parameter controls the confidence interval
)

plt.title('Log(SalePrice) vs. Total Square Footage (Regression Plot)', fontsize=14)
plt.xlabel('Total Square Footage (Total SF)', fontsize=12)
plt.ylabel('Log(SalePrice)', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.savefig('total_sf_regplot.png')



df_train.info()


df_train['Age'] = df['Yr Sold'] - df['Year Built']
df_train['year_since_remod'] = df['Yr Sold'] - df['Year Remod/Add']


df_train


# Define the list of highly skewed columns to be transformed (excluding target, already done, and age features)
final_skewed_cols = [
    'Low Qual Fin SF', 'Kitchen AbvGr', 'BsmtFin SF 2', 'Bsmt Half Bath',
    'Mas Vnr Area', 'Open Porch SF', 'Wood Deck SF', 'MS SubClass',
    'Lot Frontage', 'Gr Liv Area', '1st Flr SF', 'Bsmt Unf SF',
    '2nd Flr SF', 'BsmtFin SF 1'
]

# Apply the log1p transformation to each column, overwriting the original values
for col in final_skewed_cols:
    df_train[col] = np.log1p(df[col])

# Check the skewness of a few transformed columns to confirm the effect
# Notice how Mas Vnr Area dropped from 2.60 to 0.54!
print("--- Skewness Check After log1p Transformation ---")
print(df_train[final_skewed_cols].apply(lambda x: x.skew()).head(5).to_string())

# Save the dataset with log-transformed features.
df_train.to_csv("AmesHousing_Log_Transformed_Features.csv", index=False)


df1 = df_train
df1['Sale Price'] = df['SalePrice']


df1.to_csv("AmesHousing_cleaned_data", index = False)





split = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=42)


df1


df1['Sale_price_cat'] = pd.cut(df1['Sale Price'],bins = [9.4550,11.7280,11.8590,12.0910,12.3460, np.inf],labels = [1,2,3,4,5])


for train_index, test_index in split.split(df1,df1['Sale_price_cat']):
    df1.loc[test_index].drop('Sale_price_cat',axis = 1).to_csv('test_input.csv', index = False)
    df1.loc[train_index].drop('Sale_price_cat',axis = 1).to_csv('train_input.csv', index = False)



